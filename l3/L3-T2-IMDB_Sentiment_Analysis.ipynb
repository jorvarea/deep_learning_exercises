{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ODxhL8wWSUqP"
      },
      "source": [
        "# Sentiment Analysis\n",
        "In this notebook we will build a complete sentiment analysis model using the IMDB movie review dataset to classify them as either positive or negative.\n",
        "\n",
        "We will follow these steps:\n",
        "1.  **Import Libraries:** Load all necessary packages.\n",
        "2.  **Load Data:** Read the `IMDB Dataset10M.csv` files.\n",
        "3.  **Preprocess Text:** Clean the text (remove HTML, punctuation, stopwords) and tokenize it.\n",
        "4.  **Build Vocabulary:** Create a word-to-index mapping from our training data.\n",
        "5.  **Create PyTorch Datasets & DataLoaders:** Convert our data into a format PyTorch can use, including padding and numericalization.\n",
        "6.  **Define the LSTM Model:** Create the neural network architecture, defining which embedding method is going to be use: Learnt Embedding or Glove trained embeddings (file \"glove.6B.100d.txt\" needed).\n",
        "7.  **Train the Model:** Feed the data to the model and update its weights.\n",
        "8.  **Evaluate the Model:** Check how well our model performs on unseen test data.\n",
        "9.  **Run Inference:** Use the trained model to predict the sentiment of new, custom reviews."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-GgKCVrNSUqT"
      },
      "source": [
        "## Step 1: Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "riCIXJxlSUqT",
        "outputId": "ffe00bfd-f2cf-414e-acc5-ef9cb87a9d83"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from collections import Counter\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Download NLTK data (if not already downloaded)\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt_tab') # Added to download the missing resource"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nV9u_uwdSUqU"
      },
      "source": [
        "## Step 2: Load Data\n",
        "\n",
        "We'll load our training and testing data from the CSV files using pandas. The `imdb_tr.csv` file will be used for both training and validation, and `test/imdb_te.csv` will be reserved for our final test."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2eEc59ZZQHbd",
        "outputId": "3f3e75c7-c5c8-4ba0-b525-34420899eaae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Data Head:\n",
            "                                                text  s\n",
            "0  One of the other reviewers has mentioned that ...  1\n",
            "1  A wonderful little production. <br /><br />The...  1\n",
            "2  I thought this was a wonderful way to spend ti...  1\n",
            "3  Basically there's a family where a little boy ...  0\n",
            "4  Petter Mattei's \"Love in the Time of Money\" is...  1\n",
            "Total samples in IMDB Dataset: 10000\n",
            "\n",
            "Train Data Head:\n",
            "                                                   text  s\n",
            "9291  A bit \"the movie in the movie\" case, or as the...  1\n",
            "6051  This utterly dull, senseless, pointless, spiri...  0\n",
            "3778  Directed by E. Elias Merhige \"Begotten\" is an ...  0\n",
            "4739  For getting so many positive reviews, this mov...  0\n",
            "2973  When I first heard about the show, I heard a l...  1\n",
            "\n",
            "Test Data Head:\n",
            "                                                   text  s\n",
            "9399  This is one of the best of the series, ranking...  1\n",
            "8503  Such energy and vitality. You just can't go wr...  1\n",
            "621   The jokes are obvious, the gags are corny, and...  1\n",
            "7017  A good film with--for its time--an intense, sp...  1\n",
            "3491  Well, I'm an Italian horror big fan and I love...  1\n",
            "\n",
            "Total training samples: 4000 (40.00%) \n",
            "Total test samples: 6000 (60.00%) \n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# 1. Load the IMDB Dataset.csv file\n",
        "try:\n",
        "    # Use engine='python' for better handling of parsing errors and on_bad_lines='warn' to log issues\n",
        "    imdb_df = pd.read_csv('/content/IMDB Dataset10M.csv', engine='python', on_bad_lines='warn')\n",
        "    # The original notebook uses a column named 's' for sentiment. If 'IMDB Dataset.csv' has 'sentiment', rename it.\n",
        "    if 'sentiment' in imdb_df.columns and 's' not in imdb_df.columns:\n",
        "        imdb_df = imdb_df.rename(columns={'sentiment': 's'})\n",
        "    # Convert sentiment to numerical labels (0 for negative, 1 for positive)\n",
        "    imdb_df['s'] = imdb_df['s'].map({'negative': 0, 'positive': 1})\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: 'IMDB Dataset.csv' not found. Creating a dummy DataFrame for demonstration.\")\n",
        "    # Create a dummy DataFrame with sample data to prevent further errors\n",
        "    data = {\n",
        "        'review': [\n",
        "            \"This movie was absolutely fantastic, a must-watch for everyone!\",\n",
        "            \"I truly hated this film, it was a complete waste of my time.\",\n",
        "            \"It was an okay experience, nothing extraordinary, but not bad either.\",\n",
        "            \"A brilliant performance by the lead actor, very inspiring and moving.\",\n",
        "            \"The plot was incredibly predictable and the acting was just terrible.\",\n",
        "            \"Highly recommend this, I was on the edge of my seat the whole time.\",\n",
        "            \"So boring, I nearly fell asleep within the first fifteen minutes.\",\n",
        "            \"Loved every single minute of it, definitely going to watch it again.\",\n",
        "            \"Quite a disappointing experience, I expected much more from this.\",\n",
        "            \"Mediocre at best, could have been significantly better in many aspects.\",\n",
        "            \"What a masterpiece! The cinematography was stunning.\",\n",
        "            \"This is the worst movie I've seen all year. Avoid at all costs.\",\n",
        "            \"A decent effort, but it lacked a certain spark to make it great.\",\n",
        "            \"Absolutely captivated me from start to finish, truly remarkable.\",\n",
        "            \"Such a dull and unengaging story, felt like an eternity watching it.\"\n",
        "        ],\n",
        "        'sentiment': [\n",
        "            \"positive\", \"negative\", \"positive\", \"positive\", \"negative\",\n",
        "            \"positive\", \"negative\", \"positive\", \"negative\", \"negative\",\n",
        "            \"positive\", \"negative\", \"positive\", \"positive\", \"negative\"\n",
        "        ]\n",
        "    }\n",
        "    imdb_df = pd.DataFrame(data)\n",
        "    # Ensure dummy data matches expected processing: rename sentiment to s and convert to numerical\n",
        "    imdb_df = imdb_df.rename(columns={'sentiment': 's'})\n",
        "    imdb_df['s'] = imdb_df['s'].map({'negative': 0, 'positive': 1})\n",
        "\n",
        "# Assuming the text column is named 'review' in 'IMDB Dataset.csv', rename to 'text' for consistency\n",
        "if 'review' in imdb_df.columns and 'text' not in imdb_df.columns:\n",
        "    imdb_df = imdb_df.rename(columns={'review': 'text'})\n",
        "\n",
        "print(\"Original Data Head:\")\n",
        "print(imdb_df.head())\n",
        "print(f\"Total samples in IMDB Dataset: {len(imdb_df)}\")\n",
        "\n",
        "# 2. Split imdb_df into train_val and test. Default values (80%/20%).\n",
        "train_val_df, test_df = train_test_split(imdb_df,\n",
        "                                        test_size=0.6, #def 0.2\n",
        "                                        random_state=42,\n",
        "                                        stratify=imdb_df['s'])\n",
        "\n",
        "\n",
        "# 4. Print the first 5 rows of train_df, val_df, and test_df\n",
        "print(\"\\nTrain Data Head:\")\n",
        "print(train_val_df.head())\n",
        "print(\"\\nTest Data Head:\")\n",
        "print(test_df.head())\n",
        "\n",
        "# 5. Print the total number of samples in train_df, val_df, and test_df\n",
        "print(f\"\\nTotal training samples: {len(train_val_df)} ({(len(train_val_df)/len(imdb_df))*100:.2f}%) \")\n",
        "print(f\"Total test samples: {len(test_df)} ({(len(test_df)/len(imdb_df))*100:.2f}%) \")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vU7LezAkSUqV"
      },
      "source": [
        "## Step 3: Preprocess Text\n",
        "\n",
        "This is a crucial step. We need to clean the text to make it easier for the model to learn.\n",
        "Our `preprocess_text` function will:\n",
        "1.  Remove HTML tags (like `<br />`).\n",
        "2.  Remove punctuation and special characters, keeping only letters.\n",
        "3.  Convert all text to lowercase.\n",
        "4.  Tokenize the text (split it into individual words).\n",
        "5.  Remove common English stopwords (like 'a', 'the', 'is')."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oz6GZ27RSUqV",
        "outputId": "7c5c53ec-14b3-435b-a96f-3d7fb1748876"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original: This is a sample review... <br /><br />It's not bad, but it could be better! 10/10\n",
            "Processed: ['sample', 'review', 'bad', 'could', 'better']\n"
          ]
        }
      ],
      "source": [
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def preprocess_text(text):\n",
        "    # 1. Remove HTML tags\n",
        "    text = re.sub(r'<[^>]+>', '', text)\n",
        "    # 2. Remove punctuation/special chars\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', text, re.I|re.A)\n",
        "    # 3. Convert to lowercase\n",
        "    text = text.lower()\n",
        "    # 4. Tokenize\n",
        "    tokens = word_tokenize(text)\n",
        "    # 5. Remove stopwords\n",
        "    processed_tokens = [word for word in tokens if word not in stop_words]\n",
        "    return processed_tokens\n",
        "\n",
        "# Let's test the function\n",
        "sample_review = \"This is a sample review... <br /><br />It's not bad, but it could be better! 10/10\"\n",
        "print(f\"Original: {sample_review}\")\n",
        "print(f\"Processed: {preprocess_text(sample_review)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BAW7gQDkSUqW"
      },
      "source": [
        "## Step 4: Build Vocabulary\n",
        "\n",
        "Our model can't understand words; it only understands numbers. We need to create a \"vocabulary\" that maps each unique word to an integer.\n",
        "\n",
        "We'll add two special tokens:\n",
        "* `<PAD>`: A token we'll use to pad shorter reviews so all sequences in a batch have the same length.\n",
        "* `<UNK>`: A token for words that appear in our test data but not in our training data (unknown words)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K-VvfrYcSUqW",
        "outputId": "37fb36e7-4bbf-462a-a626-84f01f1807b0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Preprocessing training data (this may take a minute)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4000/4000 [00:03<00:00, 1148.45it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Vocabulary size: 10194\n",
            "First 10 vocab items: [('<PAD>', 0), ('<UNK>', 1), ('bit', 2), ('movie', 3), ('case', 4), ('theme', 5), ('virtual', 6), ('game', 7), ('reality', 8), ('even', 9)]\n"
          ]
        }
      ],
      "source": [
        "def build_vocab(data, min_freq=5):\n",
        "    word_counts = Counter()\n",
        "    for text in data:\n",
        "        word_counts.update(text)\n",
        "\n",
        "    # Create vocabulary, starting with special tokens\n",
        "    vocab = {'<PAD>': 0, '<UNK>': 1}\n",
        "\n",
        "    # Add words that meet the minimum frequency\n",
        "    idx = 2\n",
        "    for word, count in word_counts.items():\n",
        "        if count >= min_freq:\n",
        "            vocab[word] = idx\n",
        "            idx += 1\n",
        "    return vocab\n",
        "\n",
        "# Apply preprocessing to our training data before building vocab\n",
        "print(\"Preprocessing training data (this may take a minute)...\")\n",
        "# We use train_val_df['text'] to build the vocab\n",
        "processed_train_texts = [preprocess_text(text) for text in tqdm(train_val_df['text'])]\n",
        "\n",
        "# Build the vocabulary\n",
        "word2idx = build_vocab(processed_train_texts)\n",
        "vocab_size = len(word2idx)\n",
        "\n",
        "print(f\"\\nVocabulary size: {vocab_size}\")\n",
        "print(\"First 10 vocab items:\", list(word2idx.items())[:10])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cxgX6Hv8SUqX"
      },
      "source": [
        "## Step 5: Create PyTorch Datasets & DataLoaders\n",
        "\n",
        "Now we'll create a custom `SentimentDataset` class. This class will handle:\n",
        "1.  Taking a review text.\n",
        "2.  Preprocessing it.\n",
        "3.  Converting its words to indices using our `word2idx` vocabulary.\n",
        "4.  **Padding** or **truncating** the sequence so all sequences have the same `max_length`.\n",
        "\n",
        "We'll also split our `train_val_df` into a training set and a validation set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tRuIG2ZXSUqX",
        "outputId": "f598d0c0-c75a-48cb-ddcd-f966b97d42f7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train batches: 32\n",
            "Validation batches: 32\n",
            "Test batches: 94\n"
          ]
        }
      ],
      "source": [
        "class SentimentDataset(Dataset):\n",
        "    def __init__(self, texts, labels, word2idx, max_length):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.word2idx = word2idx\n",
        "        self.max_length = max_length\n",
        "        self.stop_words = set(stopwords.words('english'))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def preprocess_text(self, text):\n",
        "        text = re.sub(r'<[^>]+>', '', text)\n",
        "        text = re.sub(r'[^a-zA-Z\\s]', '', text, re.I|re.A)\n",
        "        text = text.lower()\n",
        "        tokens = word_tokenize(text)\n",
        "        processed_tokens = [word for word in tokens if word not in self.stop_words]\n",
        "        return processed_tokens\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.texts[idx]\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        processed_tokens = self.preprocess_text(text)\n",
        "\n",
        "        # Convert to indices\n",
        "        indexed_tokens = [self.word2idx.get(word, self.word2idx['<UNK>']) for word in processed_tokens]\n",
        "\n",
        "        # Pad or truncate\n",
        "        if len(indexed_tokens) < self.max_length:\n",
        "            # Pad with <PAD> token (index 0)\n",
        "            padded_tokens = indexed_tokens + [self.word2idx['<PAD>']] * (self.max_length - len(indexed_tokens))\n",
        "        else:\n",
        "            # Truncate\n",
        "            padded_tokens = indexed_tokens[:self.max_length]\n",
        "\n",
        "        return torch.tensor(padded_tokens), torch.tensor(label, dtype=torch.float32)\n",
        "\n",
        "# --- Hyperparameters ---\n",
        "MAX_LENGTH = 100 # def 200\n",
        "BATCH_SIZE = 64  # def 64\n",
        "\n",
        "# Split training data into train and validation sets\n",
        "train_df, val_df = train_test_split(train_val_df, test_size=0.5, random_state=42, stratify=train_val_df['s'])\n",
        "\n",
        "# Create Datasets\n",
        "train_dataset = SentimentDataset(train_df['text'].tolist(), train_df['s'].tolist(), word2idx, MAX_LENGTH)\n",
        "val_dataset = SentimentDataset(val_df['text'].tolist(), val_df['s'].tolist(), word2idx, MAX_LENGTH)\n",
        "test_dataset = SentimentDataset(test_df['text'].tolist(), test_df['s'].tolist(), word2idx, MAX_LENGTH)\n",
        "\n",
        "# Create DataLoaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "print(f\"Train batches: {len(train_loader)}\")\n",
        "print(f\"Validation batches: {len(val_loader)}\")\n",
        "print(f\"Test batches: {len(test_loader)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pPvpIbB2SUqX"
      },
      "source": [
        "## Step 6: Define the LSTM Model and choose your embedding (random learnt or glove)\n",
        "\n",
        "Time to build our network! It will have:\n",
        "1.  `nn.Embedding`: Turns our word indices into dense vectors (embeddings).\n",
        "2.  `nn.LSTM`: The main recurrent layer that processes the sequence.\n",
        "3.  `nn.Linear`: A standard fully-connected layer to give us a final score.\n",
        "4.  `nn.Sigmoid`: (Applied in the forward pass) To squash the score between 0 and 1."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# 1. SAFE GloVe Loader (skips malformed lines)\n",
        "# ---------------------------------------------------------\n",
        "def load_glove_embeddings(glove_path, word2idx, embedding_dim):\n",
        "    \"\"\"\n",
        "    Loads pretrained GloVe vectors into a matrix aligned with word2idx.\n",
        "    - Skips malformed or corrupted lines\n",
        "    - Prints how many lines were invalid\n",
        "    \"\"\"\n",
        "    vocab_size = len(word2idx)\n",
        "\n",
        "    # Initialize with random embeddings for missing words\n",
        "    embedding_matrix = np.random.uniform(\n",
        "        -0.05, 0.05, (vocab_size, embedding_dim)\n",
        "    ).astype(np.float32)\n",
        "\n",
        "    print(\"Loading GloVe embeddings safely...\")\n",
        "    found = 0\n",
        "    skipped = 0\n",
        "\n",
        "    with open(glove_path, \"r\", encoding=\"utf8\") as f:\n",
        "        for i, line in enumerate(f):\n",
        "            parts = line.strip().split()\n",
        "\n",
        "            # A correct line must contain 1 word + embedding_dim values\n",
        "            if len(parts) != embedding_dim + 1:\n",
        "                skipped += 1\n",
        "                continue\n",
        "\n",
        "            word = parts[0]\n",
        "            try:\n",
        "                vector = np.asarray(parts[1:], dtype=np.float32)\n",
        "            except ValueError:\n",
        "                skipped += 1\n",
        "                continue\n",
        "\n",
        "            if word in word2idx:\n",
        "                idx = word2idx[word]\n",
        "                embedding_matrix[idx] = vector\n",
        "                found += 1\n",
        "\n",
        "    print(f\"✔ Loaded {found} GloVe vectors.\")\n",
        "    print(f\"⚠ Skipped {skipped} malformed lines.\")\n",
        "    print(f\"Embedding matrix shape: {embedding_matrix.shape}\")\n",
        "\n",
        "    return torch.tensor(embedding_matrix)\n",
        "\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# 2. LSTM Model (supports GloVe or random embeddings)\n",
        "# ---------------------------------------------------------\n",
        "class SentimentLSTM(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim,\n",
        "                 n_layers, dropout, padding_idx,\n",
        "                 glove_embeddings=None,\n",
        "                 freeze_embeddings=False):\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        # ----------- EMBEDDING LAYER -----------\n",
        "        self.embedding = nn.Embedding(\n",
        "            vocab_size,\n",
        "            embedding_dim,\n",
        "            padding_idx=padding_idx\n",
        "        )\n",
        "\n",
        "        if USE_GLOVE == 1:\n",
        "            if glove_embeddings is None:\n",
        "                raise ValueError(\"USE_GLOVE is 1 but glove_embeddings is None.\")\n",
        "\n",
        "            print(\"Using pretrained GloVe embeddings...\")\n",
        "            self.embedding.weight.data.copy_(glove_embeddings)\n",
        "\n",
        "            if freeze_embeddings:\n",
        "                self.embedding.weight.requires_grad = False\n",
        "                print(\"GloVe embeddings are FROZEN (not trainable).\")\n",
        "            else:\n",
        "                print(\"GloVe embeddings will be FINE-TUNED.\")\n",
        "\n",
        "        else:\n",
        "            print(\"Using randomly initialized embeddings (train from scratch).\")\n",
        "\n",
        "        # ----------- LSTM LAYER -----------\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=embedding_dim,\n",
        "            hidden_size=hidden_dim,\n",
        "            num_layers=n_layers,\n",
        "            bidirectional=True,\n",
        "            dropout=dropout,\n",
        "            batch_first=True\n",
        "        )\n",
        "\n",
        "        # ----------- DROPOUT -----------\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        # ----------- OUTPUT LAYER -----------\n",
        "        self.fc = nn.Linear(hidden_dim * 2, output_dim)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "\n",
        "    def forward(self, text_batch):\n",
        "\n",
        "        embedded = self.embedding(text_batch)\n",
        "        lstm_out, (hidden, cell) = self.lstm(embedded)\n",
        "\n",
        "        # Last forward and backward hidden states\n",
        "        forward_hidden = hidden[-2, :, :]\n",
        "        backward_hidden = hidden[-1, :, :]\n",
        "\n",
        "        hidden_cat = torch.cat((forward_hidden, backward_hidden), dim=1)\n",
        "\n",
        "        dropped = self.dropout(hidden_cat)\n",
        "        logits = self.fc(dropped)\n",
        "        output = self.sigmoid(logits)\n",
        "\n",
        "        return output.squeeze()\n",
        "\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# 3. MODEL HYPERPARAMETERS\n",
        "# ---------------------------------------------------------\n",
        "EMBEDDING_DIM = 100\n",
        "HIDDEN_DIM = 128\n",
        "OUTPUT_DIM = 1\n",
        "N_LAYERS = 2\n",
        "DROPOUT = 0.5\n",
        "PADDING_IDX = word2idx[\"<PAD>\"]\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# 4. CHOOSE EMBEDDING OPTION\n",
        "# ---------------------------------------------------------\n",
        "USE_GLOVE = 1     # 0 = random, 1 = GloVe\n",
        "# FREEZE = False    # freeze GloVe weights?\n",
        "FREEZE = True    # freeze GloVe weights?\n",
        "\n",
        "if USE_GLOVE == 1:\n",
        "    glove_path = \"/content/glove.6B.100d.txt\"\n",
        "    glove_embeddings = load_glove_embeddings(glove_path, word2idx, EMBEDDING_DIM)\n",
        "else:\n",
        "    glove_embeddings = None\n",
        "\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# 5. INSTANTIATE MODEL\n",
        "# ---------------------------------------------------------\n",
        "model = SentimentLSTM(\n",
        "    vocab_size=vocab_size,\n",
        "    embedding_dim=EMBEDDING_DIM,\n",
        "    hidden_dim=HIDDEN_DIM,\n",
        "    output_dim=OUTPUT_DIM,\n",
        "    n_layers=N_LAYERS,\n",
        "    dropout=DROPOUT,\n",
        "    padding_idx=PADDING_IDX,\n",
        "    glove_embeddings=glove_embeddings,\n",
        "    freeze_embeddings=FREEZE\n",
        ")\n",
        "\n",
        "print(model)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OBfkQZTiYRzk",
        "outputId": "ecb98cde-7137-4140-90a7-ff0ce5c145c4"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading GloVe embeddings safely...\n",
            "✔ Loaded 9956 GloVe vectors.\n",
            "⚠ Skipped 1 malformed lines.\n",
            "Embedding matrix shape: (10194, 100)\n",
            "Using pretrained GloVe embeddings...\n",
            "GloVe embeddings are FROZEN (not trainable).\n",
            "SentimentLSTM(\n",
            "  (embedding): Embedding(10194, 100, padding_idx=0)\n",
            "  (lstm): LSTM(100, 128, num_layers=2, batch_first=True, dropout=0.5, bidirectional=True)\n",
            "  (dropout): Dropout(p=0.5, inplace=False)\n",
            "  (fc): Linear(in_features=256, out_features=1, bias=True)\n",
            "  (sigmoid): Sigmoid()\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CJe9LiUDSUqY"
      },
      "source": [
        "## Step 7: Train the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A_VTURXZSUqY",
        "outputId": "f41ac3a7-17f2-4117-a7b2-514d6bbe3e3c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training on device: cpu\n",
            "Starting training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/5 [Train]: 100%|██████████| 32/32 [00:34<00:00,  1.09s/it]\n",
            "Epoch 1/5 [Val]: 100%|██████████| 32/32 [00:11<00:00,  2.72it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 01 | Train Loss: 0.680 | Train Acc: 56.84% | Val. Loss: 0.633 | Val. Acc: 64.55%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/5 [Train]: 100%|██████████| 32/32 [00:40<00:00,  1.26s/it]\n",
            "Epoch 2/5 [Val]: 100%|██████████| 32/32 [00:12<00:00,  2.50it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 02 | Train Loss: 0.586 | Train Acc: 70.31% | Val. Loss: 0.589 | Val. Acc: 70.56%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/5 [Train]: 100%|██████████| 32/32 [00:32<00:00,  1.02s/it]\n",
            "Epoch 3/5 [Val]: 100%|██████████| 32/32 [00:11<00:00,  2.78it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 03 | Train Loss: 0.553 | Train Acc: 73.39% | Val. Loss: 0.531 | Val. Acc: 74.76%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4/5 [Train]: 100%|██████████| 32/32 [00:31<00:00,  1.01it/s]\n",
            "Epoch 4/5 [Val]: 100%|██████████| 32/32 [00:11<00:00,  2.67it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 04 | Train Loss: 0.516 | Train Acc: 75.24% | Val. Loss: 0.520 | Val. Acc: 75.34%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5/5 [Train]: 100%|██████████| 32/32 [00:31<00:00,  1.00it/s]\n",
            "Epoch 5/5 [Val]: 100%|██████████| 32/32 [00:11<00:00,  2.82it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 05 | Train Loss: 0.477 | Train Acc: 77.83% | Val. Loss: 0.511 | Val. Acc: 76.81%\n",
            "Training finished.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "import torch # Added torch for device setup\n",
        "from tqdm import tqdm # Added tqdm import, assuming it's used in training loop\n",
        "\n",
        "# --- Training Hyperparameters ---\n",
        "LEARNING_RATE = 0.001 #def:0.001\n",
        "N_EPOCHS = 5 #def:5\n",
        "\n",
        "# Loss function and optimizer\n",
        "criterion = nn.BCELoss() # Binary Cross Entropy Loss\n",
        "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "# Check for GPU\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Training on device: {device}\")\n",
        "\n",
        "model = model.to(device)\n",
        "criterion = criterion.to(device)\n",
        "\n",
        "def get_accuracy(preds, y):\n",
        "    \"\"\"Returns accuracy per batch\"\"\"\n",
        "    # Round predictions to the closest integer (0 or 1)\n",
        "    rounded_preds = torch.round(preds)\n",
        "    correct = (rounded_preds == y).float()\n",
        "    acc = correct.sum() / len(correct)\n",
        "    return acc\n",
        "\n",
        "print(\"Starting training...\")\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "\n",
        "    train_loss = 0.0\n",
        "    train_acc = 0.0\n",
        "\n",
        "    # --- Training Phase ---\n",
        "    model.train() # Set model to training mode\n",
        "\n",
        "    for inputs, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{N_EPOCHS} [Train]\"):\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        # 1. Zero gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # 2. Forward pass\n",
        "        predictions = model(inputs)\n",
        "\n",
        "        # 3. Calculate loss and accuracy\n",
        "        loss = criterion(predictions, labels)\n",
        "        acc = get_accuracy(predictions, labels)\n",
        "\n",
        "        # 4. Backward pass\n",
        "        loss.backward()\n",
        "\n",
        "        # 5. Update weights\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "        train_acc += acc.item()\n",
        "\n",
        "    # --- Validation Phase ---\n",
        "    val_loss = 0.0\n",
        "    val_acc = 0.0\n",
        "\n",
        "    model.eval() # Set model to evaluation mode\n",
        "    with torch.no_grad(): # Disable gradient calculation\n",
        "        for inputs, labels in tqdm(val_loader, desc=f\"Epoch {epoch+1}/{N_EPOCHS} [Val]\"):\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            predictions = model(inputs)\n",
        "\n",
        "            loss = criterion(predictions, labels)\n",
        "            acc = get_accuracy(predictions, labels)\n",
        "\n",
        "            val_loss += loss.item()\n",
        "            val_acc += acc.item()\n",
        "\n",
        "    # Print epoch statistics\n",
        "    avg_train_loss = train_loss / len(train_loader)\n",
        "    avg_train_acc = train_acc / len(train_loader)\n",
        "    avg_val_loss = val_loss / len(val_loader)\n",
        "    avg_val_acc = val_acc / len(val_loader)\n",
        "\n",
        "    print(f'Epoch {epoch+1:02} | Train Loss: {avg_train_loss:.3f} | Train Acc: {avg_train_acc*100:.2f}% | Val. Loss: {avg_val_loss:.3f} | Val. Acc: {avg_val_acc*100:.2f}%')\n",
        "\n",
        "print(\"Training finished.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JGNinkrfSUqY"
      },
      "source": [
        "## Step 8: Evaluate the Model\n",
        "\n",
        "Now we'll use the held-out test set (`test_loader`) to see how our model generalizes to completely new data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "cn9z1-xmSUqY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "92e5b30d-0ea3-4307-ec53-93f481a8b2fc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Testing: 100%|██████████| 94/94 [00:33<00:00,  2.81it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Loss: 0.504 | Test Acc: 76.63%\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.75      0.79      0.77      2983\n",
            "    Positive       0.78      0.74      0.76      3017\n",
            "\n",
            "    accuracy                           0.77      6000\n",
            "   macro avg       0.77      0.77      0.77      6000\n",
            "weighted avg       0.77      0.77      0.77      6000\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "test_loss = 0.0\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in tqdm(test_loader, desc=\"Testing\"):\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        predictions = model(inputs)\n",
        "\n",
        "        loss = criterion(predictions, labels)\n",
        "        test_loss += loss.item()\n",
        "\n",
        "        # Store predictions and labels\n",
        "        rounded_preds = torch.round(predictions)\n",
        "        all_preds.extend(rounded_preds.cpu().numpy())\n",
        "        all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "avg_test_loss = test_loss / len(test_loader)\n",
        "test_acc = accuracy_score(all_labels, all_preds)\n",
        "\n",
        "print(f'Test Loss: {avg_test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')\n",
        "print(\"\\nClassification Report:\")\n",
        "# 0 is negative, 1 is positive\n",
        "print(classification_report(all_labels, all_preds, target_names=['Negative', 'Positive']))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ESAqtgejSUqY"
      },
      "source": [
        "## Step 9: Run Inference\n",
        "\n",
        "Let's create a final function that takes any review as a string and predicts its sentiment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "T2wVxrRqSUqY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a102c2a1-8884-4963-ed09-ca449203e3a0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Review: 'This movie was fantastic! The acting was superb and the plot was gripping. I would recommend this to everyone.'\n",
            "Sentiment: Positive (Probability: 0.6117)\n",
            "\n",
            "Review: 'What a waste of time. The plot was predictable and the acting was terrible. I would not watch this again.'\n",
            "Sentiment: Positive (Probability: 0.6107)\n"
          ]
        }
      ],
      "source": [
        "def predict_sentiment(review_text, model, word2idx, max_length, device):\n",
        "    model.eval()\n",
        "\n",
        "    # Preprocess the text\n",
        "    processed_tokens = preprocess_text(review_text)\n",
        "\n",
        "    # Convert to indices\n",
        "    indexed_tokens = [word2idx.get(word, word2idx['<UNK>']) for word in processed_tokens]\n",
        "\n",
        "    # Pad or truncate\n",
        "    if len(indexed_tokens) < max_length:\n",
        "        padded_tokens = indexed_tokens + [word2idx['<PAD>']] * (max_length - len(indexed_tokens))\n",
        "    else:\n",
        "        padded_tokens = indexed_tokens[:max_length]\n",
        "\n",
        "    # Convert to tensor and add batch dimension (batch size = 1)\n",
        "    input_tensor = torch.tensor(padded_tokens).unsqueeze(0).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        prediction = model(input_tensor)\n",
        "\n",
        "    probability = prediction.item()\n",
        "    sentiment = \"Positive\" if probability > 0.5 else \"Negative\"\n",
        "\n",
        "    return sentiment, probability\n",
        "\n",
        "# --- Test with custom reviews ---\n",
        "\n",
        "positive_review = \"This movie was fantastic! The acting was superb and the plot was gripping. I would recommend this to everyone.\"\n",
        "neg_review = \"What a waste of time. The plot was predictable and the acting was terrible. I would not watch this again.\"\n",
        "\n",
        "sentiment, prob = predict_sentiment(positive_review, model, word2idx, MAX_LENGTH, device)\n",
        "print(f\"Review: '{positive_review}'\")\n",
        "print(f\"Sentiment: {sentiment} (Probability: {prob:.4f})\\n\")\n",
        "\n",
        "sentiment, prob = predict_sentiment(neg_review, model, word2idx, MAX_LENGTH, device)\n",
        "print(f\"Review: '{neg_review}'\")\n",
        "print(f\"Sentiment: {sentiment} (Probability: {prob:.4f})\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}