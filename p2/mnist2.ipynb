{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a31e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "340d61d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "print(\"Datos de entrenamiento\", x_train.shape)\n",
    "print(\"Datos de test\", x_test.shape)\n",
    "print(\"Primeros dígitos: \", y_train[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76b15932",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train / 255.0\n",
    "x_test = x_test / 255.0\n",
    "y_train = (y_train % 2).astype(np.int32)\n",
    "y_test = (y_test % 2).astype(np.int32)\n",
    "print(\"Etiquetas paridad (0=par, 1=impar):\", y_train[:5])\n",
    "x_train.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a02aeb7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def build_model(hidden_units: int) -> tf.keras.models.Sequential:\n",
    "    model = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "        tf.keras.layers.Dense(hidden_units, activation='relu'),\n",
    "        tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "\n",
    "def get_optimizer(name: str, learning_rate: float) -> tf.keras.optimizers.Optimizer:\n",
    "    normalized_name: str = name.lower()\n",
    "    if normalized_name == 'sgd':\n",
    "        return tf.keras.optimizers.SGD(learning_rate=learning_rate)\n",
    "    if normalized_name == 'adagrad':\n",
    "        return tf.keras.optimizers.Adagrad(learning_rate=learning_rate)\n",
    "    if normalized_name == 'rmsprop':\n",
    "        return tf.keras.optimizers.RMSprop(learning_rate=learning_rate)\n",
    "    if normalized_name == 'adam':\n",
    "        return tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    raise ValueError(f\"Optimizer {name} not supported\")\n",
    "\n",
    "\n",
    "experiments: list[dict[str, object]] = [\n",
    "    {\n",
    "        'name': 'hidden_64',\n",
    "        'hidden_units': 64,\n",
    "        'epochs': 10,\n",
    "        'batch_size': 32,\n",
    "        'loss': 'binary_crossentropy',\n",
    "        'optimizer': 'sgd',\n",
    "        'learning_rate': 0.01,\n",
    "        'validation_split': 0.1,\n",
    "    },\n",
    "    {\n",
    "        'name': 'hidden_128',\n",
    "        'hidden_units': 128,\n",
    "        'epochs': 10,\n",
    "        'batch_size': 32,\n",
    "        'loss': 'binary_crossentropy',\n",
    "        'optimizer': 'sgd',\n",
    "        'learning_rate': 0.01,\n",
    "        'validation_split': 0.1,\n",
    "    },\n",
    "    {\n",
    "        'name': 'hidden_256',\n",
    "        'hidden_units': 256,\n",
    "        'epochs': 10,\n",
    "        'batch_size': 32,\n",
    "        'loss': 'binary_crossentropy',\n",
    "        'optimizer': 'sgd',\n",
    "        'learning_rate': 0.01,\n",
    "        'validation_split': 0.1,\n",
    "    },\n",
    "    {\n",
    "        'name': 'hidden_512',\n",
    "        'hidden_units': 512,\n",
    "        'epochs': 10,\n",
    "        'batch_size': 32,\n",
    "        'loss': 'binary_crossentropy',\n",
    "        'optimizer': 'sgd',\n",
    "        'learning_rate': 0.01,\n",
    "        'validation_split': 0.1,\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae9aedec",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "history_objects: list[tf.keras.callbacks.History] = []\n",
    "train_times: list[float] = []\n",
    "train_losses: list[float] = []\n",
    "train_accuracies: list[float] = []\n",
    "test_losses: list[float] = []\n",
    "test_accuracies: list[float] = []\n",
    "experiment_names: list[str] = []\n",
    "num_classes: int = 2\n",
    "class_labels: list[str] = ['Par', 'Impar']\n",
    "avg_proba_matrices: dict[str, np.ndarray] = {}\n",
    "\n",
    "for experiment in experiments:\n",
    "    experiment_name: str = str(experiment['name'])\n",
    "    hidden_units: int = int(experiment['hidden_units'])\n",
    "    epoch_count: int = int(experiment['epochs'])\n",
    "    batch_size: int = int(experiment['batch_size'])\n",
    "    loss_identifier: str = str(experiment['loss'])\n",
    "    optimizer_name: str = str(experiment['optimizer'])\n",
    "    learning_rate_value: float = experiment.get('learning_rate')\n",
    "    validation_split_raw: float = experiment.get('validation_split', 0.0)\n",
    "    validation_split_value: float = float(validation_split_raw)\n",
    "\n",
    "    print(f\"Iniciando experimento: {experiment_name}\")\n",
    "    print(\n",
    "        f\"Config -> hidden_units={hidden_units}, epochs={epoch_count}, batch_size={batch_size}, \"\n",
    "        f\"optimizer={optimizer_name}, learning_rate={learning_rate_value}, loss={loss_identifier}, \"\n",
    "        f\"validation_split={validation_split_value}\"\n",
    "    )\n",
    "\n",
    "    model = build_model(hidden_units)\n",
    "    optimizer_instance = get_optimizer(optimizer_name, learning_rate_value)\n",
    "    model.compile(\n",
    "        optimizer=optimizer_instance,\n",
    "        loss=loss_identifier,\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "\n",
    "    start_time: float = time.time()\n",
    "    fit_kwargs: dict[str, object] = {\n",
    "        'epochs': epoch_count,\n",
    "        'batch_size': batch_size,\n",
    "        'verbose': 1,\n",
    "    }\n",
    "    fit_kwargs['validation_split'] = validation_split_value\n",
    "    history = model.fit(x_train, y_train, **fit_kwargs)\n",
    "    end_time: float = time.time()\n",
    "    elapsed_time: float = end_time - start_time\n",
    "\n",
    "    print(f\"Training time for {experiment_name}: {elapsed_time:.2f} seconds\")\n",
    "    train_times.append(elapsed_time)\n",
    "    experiment_names.append(experiment_name)\n",
    "    history_objects.append(history)\n",
    "\n",
    "    print(\"Evaluación en entrenamiento\")\n",
    "    train_loss, train_accuracy = model.evaluate(x_train, y_train, verbose=0)\n",
    "    train_losses.append(train_loss)\n",
    "    train_accuracies.append(train_accuracy)\n",
    "\n",
    "    print(\"Evaluación en test\")\n",
    "    test_loss, test_accuracy = model.evaluate(x_test, y_test, verbose=0)\n",
    "    test_losses.append(test_loss)\n",
    "    test_accuracies.append(test_accuracy)\n",
    "\n",
    "    probs: np.ndarray = model.predict(x_test, verbose=0).reshape(-1, 1)\n",
    "    binary_probs: np.ndarray = np.concatenate((1.0 - probs, probs), axis=1)\n",
    "    avg_matrix: np.ndarray = np.zeros((num_classes, binary_probs.shape[1]), dtype=np.float64)\n",
    "    for true_label in range(num_classes):\n",
    "        mask: np.ndarray = (y_test == true_label)\n",
    "        if np.any(mask):\n",
    "            avg_matrix[true_label] = binary_probs[mask].mean(axis=0)\n",
    "        else:\n",
    "            avg_matrix[true_label] = 0.0\n",
    "    avg_proba_matrices[experiment_name] = avg_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cf0e12f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for index, history in enumerate(history_objects):\n",
    "    experiment_name = experiment_names[index]\n",
    "    print(experiment_name)\n",
    "    print(f\"train_times[{index}]: {train_times[index]:.2f} seconds\")\n",
    "    print(f\"train_losses[{index}]: {train_losses[index]:.2f}\")\n",
    "    print(f\"train_accuracies[{index}]: {train_accuracies[index]:.2f}\")\n",
    "    print(f\"test_losses[{index}]: {test_losses[index]:.2f}\")\n",
    "    print(f\"test_accuracies[{index}]: {test_accuracies[index]:.2f}\")\n",
    "\n",
    "    plt.figure(figsize=(12, 4))\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history['loss'], label='Train')\n",
    "    if 'val_loss' in history.history:\n",
    "        plt.plot(history.history['val_loss'], label='Val')\n",
    "    plt.title('Pérdida del Modelo')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Época')\n",
    "    if 'val_loss' in history.history:\n",
    "        plt.legend()\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history['accuracy'], label='Train')\n",
    "    if 'val_accuracy' in history.history:\n",
    "        plt.plot(history.history['val_accuracy'], label='Val')\n",
    "    plt.title('Precisión del Modelo')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xlabel('Época')\n",
    "    if 'val_accuracy' in history.history:\n",
    "        plt.legend()\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    M: np.ndarray = avg_proba_matrices[experiment_name]\n",
    "    print(\"Matriz de probabilidades promedio (columnas: Par, Impar):\")\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    im = plt.imshow(M, cmap='viridis', aspect='auto', vmin=0.0, vmax=1.0)\n",
    "    plt.colorbar(im, label='Prob. promedio')\n",
    "    plt.title(f'Matriz de probabilidad promedio - {experiment_name}')\n",
    "    plt.xlabel('Predicción')\n",
    "    plt.ylabel('Etiqueta verdadera')\n",
    "    plt.xticks(range(num_classes), class_labels)\n",
    "    plt.yticks(range(num_classes), class_labels)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fed4696",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def plot_metric(metric: str, title: str, ylabel: str) -> None:\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    for index, history in enumerate(history_objects):\n",
    "        experiment_name = experiment_names[index]\n",
    "        plt.plot(history.history[metric], label=f\"{experiment_name} ({metric} train)\")\n",
    "        if f\"val_{metric}\" in history.history:\n",
    "            plt.plot(history.history[f\"val_{metric}\"], linestyle='--', label=f\"{experiment_name} ({metric} val)\")\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Época')\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_metric('loss', 'Comparativa de pérdida', 'Loss')\n",
    "plot_metric('accuracy', 'Comparativa de accuracy', 'Accuracy')"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
